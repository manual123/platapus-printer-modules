"""
The server object schedules all actions that the machine can take.  At its
core is a "GeneratorManager" that is comprised of several sub-generators.
Some examples of sub-generators the GeneratorManager contain are:

    * EPoll
    * HTTP Server
    * UDP Server
    * MachineManager
"""

import base64
import copy
import contextlib
import datetime
import errno
import json
import os
import queue as Queue  # we have lots of local vars named queue
import threading
import time
import urllib.parse
import ssl

import mbcoreutils.tool_mappings

import kaiten.address
import kaiten.auth
import kaiten.bwcamera
import kaiten.constants
import kaiten.connection
import kaiten.dbus
import kaiten.debug
import kaiten.decorator
import kaiten.error
import kaiten.firmwareupdates
import kaiten.fontdownload
import kaiten.generators
import kaiten.http.client
import kaiten.jsonrpc
import kaiten.log
import kaiten.machine_manager
import kaiten.mixpanel
import kaiten.netlog
import kaiten.profiler
import kaiten.reflector
import kaiten.scheduler
import kaiten.thingiverse
import kaiten.transfer
import kaiten.udp_server
import kaiten.usb
import kaiten.util

if kaiten.constants.mock:
    import kaiten.mock

# TODO: find all code that expects IOGenerator to be defined in the server
from kaiten.generators import IOGenerator

def get_connection_fileno(connection):
    """
    Used as a key to lookup this connection in various dicts
    """
    return connection.fileno()

class Server:
    def __init__(self, config):
        self._log = kaiten.log.getlogger(self)
        self._event_log = kaiten.log.getlogger(self, base_logger_name='user_event')
        self._config = config

        self._event_log.info(
            "System Startup\n    Firmware Version: %s\n" +
            "    MAC Address: %s\n    iSerial: %s\n",
            kaiten.util.get_fw_version(),
            kaiten.util.get_mac_address(),
            kaiten.usb.get_serial())

        self._log.info("Kaiten starting with firmware version %s",
            kaiten.util.get_fw_version())
        self._log_serial_dict()
        self._stop = False # If this is ever True, we stop our main loop
        self._net_change_callbacks = []
        self._process = None
        self._clients = {}
        self._iopoll = kaiten.scheduler.IOPollGenerator(self._handle_io_error)
        generator_manager_class = kaiten.scheduler.ContractGeneratorManager
        self._generator_manager = generator_manager_class(self._iopoll)
        self._lcd_client = None
        self._fcgi_client = None
        self._transfer_callbacks = {}
        self._jsonrpc_installers = [self] # List of files used to "install" jsonrpc funcs
        # These tasks are done when the server exits its main loop
        self._cleanup_tasks = []
        self._reload_config_callbacks = []
        def reload_config(config):
            self._log.info("Kaiten reloading config")
            self._config = config
            self._has_been_connected_to = self._config["kaiten"]["has_been_connected_to"]
        self._reload_config_callbacks.append(reload_config)
        self._system_status_generator = ExtruderStatusGenerator(self)
        self._has_been_connected_to = self._config["kaiten"]["has_been_connected_to"]

        def get_server_info():
            # We need to do this every time to support starting with no iternet
            # or our internet going down
            return {
                "firmware_version": dict(kaiten.util.get_fw_version()),
                "ip": self.get_ip(),
                "machine_name": self._config["kaiten"]["machine_name"],
                "machine_type": self._config["kaiten"]["machine_type"],
                "bot_type": self._config["bot_type"],
                "has_been_connected_to": self._has_been_connected_to,
            }
        self.register_status_callback(get_server_info)
        self.add_contract_generator(self._system_status_generator)
        self._net_socket_manager = kaiten.connection.SocketManager()
        self._ssl_context = None

        # Debug access
        self._debug = kaiten.debug.Debug(self)
        self._jsonrpc_installers.append(self._debug)

        # DNS requests get their own thread because they are
        # such a giant pain to make non blocking.
        self._dns_thread = DNSThread()
        self._dns_thread.start()
        self._cleanup_tasks.append(lambda: self._dns_thread.stop())

        # Used by both the dns thread and by dbus
        self._thread_queue = ThreadGeneratorQueue()
        self._iopoll.register(self._thread_queue)

        # Authentication
        self.auth_store = kaiten.auth.AuthStore(self)
        self._jsonrpc_installers.append(self.auth_store)

        # Add components to kaiten
        # Add machine manager, which handles most of the machine iterations
        if self._config["kaiten"]["components"]["machine_manager"]:
            pass #self._log.debug("Starting machine manager")
            self._machine_manager = kaiten.machine_manager.MachineManager(
                self, self._config, self._register_process_callback)
            self.register_status_callback(self._machine_manager.get_info_dict)
            self.add_contract_generator(self._machine_manager)
            self._jsonrpc_installers.append(self._machine_manager)
            self._reload_config_callbacks.append(self._machine_manager.reload_config)
            self._cleanup_tasks.append(self._machine_manager.cleanup_machine)

        # Add the IOPoll generator to the list of kaiten generators
        if not self._config["kaiten"]["components"]["epoll"]:
            self._log.error('kaiten does not run with epoll disabled')
            raise Exception('kaiten does not run with epoll disabled')

        # Add the usb listener.  The usb_listener is saved as a state
        # variable so the dianostics process can send messages directly
        # to the LCD
        if self._config["kaiten"]["components"]["usb"]:
            usb_path = kaiten.constants.usb_path
            try:
                usb_listener = kaiten.usb.USBListener(self, usb_path)
                self._log.info("Listening to usb descriptor at address %s",
                               usb_path)
            except Exception as e:
                self._log.info("Error listening to USB at path %s",
                               usb_path, exc_info=True)
            else:
                # USB connections/disconnects are weird, see docstrings
                # for USBListener, USBConnection, USBTimeoutGenerator
                self._iopoll.register(usb_listener)

        # Add listener for TCP Socket; used by clients like conveyor
        if self._config["kaiten"]["components"]["tcp_socket"]:
            # Try to create the TCP server
            try:
                tcp_address = kaiten.address.TcpAddress(
                    kaiten.constants.tcp_address,
                    kaiten.constants.tcp_port)
                tcp_listener = tcp_address.listen()
            except Exception as e:
                self._log.info("Error starting TCP socket.", exc_info=True)
            # Add the TCP server
            else:
                tcp_generator = self._accept_tcp(tcp_listener)
                self._iopoll.register(
                    IOGenerator(tcp_generator, tcp_listener, True, False))

        # Add a separate TCP listener with SSL
        if self._config["kaiten"]["components"]["ssl_tcp_socket"]:
            # Try to create the TCP server
            try:
                ssl_tcp_address = kaiten.address.TcpAddress(
                    kaiten.constants.tcp_address,
                    kaiten.constants.ssl_tcp_port)
                ssl_tcp_listener = ssl_tcp_address.listen()
            except Exception as e:
                self._log.info("Error starting SSL TCP socket.", exc_info=True)
            # Add the TCP server
            else:
                ssl_tcp_generator = self._accept_tcp(ssl_tcp_listener, ssl=True)
                self._iopoll.register(
                    IOGenerator(ssl_tcp_generator, ssl_tcp_listener, True, False))

        # Add unix pipe; used by the LCD, queens, fcgi server
        if kaiten.constants.pipe != "":
            # Try to create the unix pipe
            try:
                pipe_address = kaiten.constants.pipe
                pipe_listener = kaiten.address.Address.address_factory(pipe_address).listen()
            except Exception as e:
                self._log.info("Error creating pipe socket at %s",
                    pipe_address, exc_info=True)
            # Add the unix pipe
            else:
                pipe_generator = self._accept_pipe(pipe_listener)
                self._iopoll.register(IOGenerator(pipe_generator, pipe_listener, True, False))
                self._cleanup_tasks.append(lambda: os.unlink(pipe_listener._path))

        # Add the dbus client, which handles all of our network management
        self._network_state = kaiten.dbus.DBusManager.default_network_state()
        self.dbus_manager = None
        self._last_ifupdn_event = datetime.datetime.utcnow()
        if kaiten.constants.mock:
            ip = kaiten.mock.get_ip_address()
            if ip is not None:
                self._network_state['state'] = 'wired'
                self._network_state['ip'] = ip
        if self._config["kaiten"]["components"]["dbus"]:
            try:
                self.dbus_manager = kaiten.dbus.DBusManager(self, self._thread_queue)
            except Exception as e:
                self._log.info("Error starting DBus Manager", exc_info=True)
            else:
                self._jsonrpc_installers.append(self.dbus_manager)
                self._cleanup_tasks.append(self.dbus_manager.close)
        else:
            self._log.warning("DBus disabled; Expect no network functionality")
        self.add_net_change_callback(self.network_state_notify)

        # UDP server initialization is deferred until the internet is up
        # and the fcgi server is running
        self.udpserver = None

        # If we're a mock, we should update the UDP server with our network
        # state now. This is necessary since we don't have a mock FCGI server.
        if kaiten.constants.mock:
            self._update_udp(self._network_state)

        self._fcgi_up = False

        self._camera = None
        if self._config["kaiten"]["components"]["camera"]:
            try:
                self._camera = kaiten.bwcamera.CameraContract(self)
            except Exception as e:
                self._log.info("Error starting Camera Generator", exc_info=True)
            else:
                self.add_contract_generator(self._camera)
                self._cleanup_tasks.append(self._camera.close)

                # We want to set a nice long timeout for the camera so that
                # we don't trigger camera resets during prints, but we also
                # don't want to wait forever to initialize the camera.
                next(self._camera)

        if self._config["kaiten"]["components"]["profiling"]:
            do_full_profiling = self._config["kaiten"]["do_full_profiling"]
            profiler = kaiten.profiler.Profiler(self, do_full_profiling)
            self._generator_manager.set_profiler(profiler)
            if do_full_profiling:
                self.add_contract_generator(profiler)

        if self._config["kaiten"]["components"]["netlog"]:
            self._netlog = kaiten.netlog.Logger(self)
            self.add_contract_generator(self._netlog)
        else:
            self._netlog = None

        # setup the reflector
        if self._config["kaiten"]["components"]["reflector"]:
            self._setup_reflector()
        else:
            self._reflector = None

        # setup analytics
        self._mixpanel = kaiten.mixpanel.MixPanel(self, self._config)
        self.add_contract_generator(self._mixpanel)
        self._reload_config_callbacks.append(self._mixpanel.reload_config)

        if os.path.exists(kaiten.constants.first_firmware_boot) and \
           self._config["kaiten"]["components"]["machine_manager"]:
            self._machine_manager.firmware_cleanup()

        if os.path.exists(kaiten.constants.do_fre) and \
           self._config['kaiten']['components']['machine_manager']:
            self._log.info("Queueing setup process")
            self._machine_manager.setup_printer(enqueue=True)

        if self._config["kaiten"]["components"]["mdns"]:
            if not self.dbus_manager:
                self._log.error("MDNS bot discovery requires dbus!")
            else:
                self.dbus_manager.start_discovery()

        # Stores server's firmware list when checking for firmware updates
        FirmwareUpdates = kaiten.firmwareupdates.FirmwareUpdates
        self.firmware_updates = FirmwareUpdates(self, self._config)
        self.add_contract_generator(self.firmware_updates)
        self._reload_config_callbacks.append(self.firmware_updates.reload_config)
        self.add_net_change_callback(self.firmware_updates.net_change_callback)


        # check for font downloads
        self.add_contract_generator(kaiten.fontdownload.FontDownload(self, self._config))
        # Set up methods to call when a client disconnects
        self._client_disconnect_hooks = [self._cleanup_disconnected_client,
                                         self._log_client_disconnect]

    def _setup_reflector(self):
        if self._config["kaiten"]["components"]["reflector"]:
            self._reflector = kaiten.reflector.Reflector(self, self._config, self._machine_manager)
            self.add_contract_generator(self._reflector)
            self._reload_config_callbacks.append(self._reflector.reload_config)
            self._reflectorExecutor = self._reflector.get_reflector_executor()
            self.add_contract_generator(self._reflectorExecutor)
            self._reload_config_callbacks.append(self._reflectorExecutor.reload_config)

    def add_net_change_callback(self, callback):
        """
        Add a callback that takes two keyword arguments, state and old_state
        The callback is invoked _after_ changing self._network_state
        """
        self._net_change_callbacks.append(callback)

    def network_state_notify(self, state, old_state=None):
        """
        Send a notification about the current network state to all clients
        """
        self._log.info("notifying clients of network state: {}".format(state))
        self.notify_clients('network_state_change', {'state': state})

    def is_online(self):
        return self._network_state['state'] != 'offline'

    def get_ip(self):
        return self._network_state.get('ip', None)

    def _log_serial_dict(self):
        filepath = '/var/machine_settings/serial.json'
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r') as f:
                    serial_dict = json.load(f)
            except (ValueError, KeyError, TypeError):
                self._log.info("JSON format error: log_serial")

    def update_discovery(self, changed_name=False):
        if self._config['kaiten']['components']['mdns']\
           and self.dbus_manager:
            self.dbus_manager.update_discovery(changed_name)
        else:
            self._log.warning("mDNS is not enabled!")
    
    @contextlib.contextmanager
    def set_ifup_modifier(self, modifier):
        """
        Set a modifier for IFUP network log events
        """
        self._ifup_modifier = [modifier]
        try:
            yield
        finally:
            del self._ifup_modifier

    @contextlib.contextmanager
    def set_ifdn_modifier(self, modifier):
        """
        Set a modifier for IFDN network log events
        """
        self._ifdn_modifier = [modifier]
        try:
            yield
        finally:
            del self._ifdn_modifier

    def network_state_change(self, full_state=None, partial_state=None):
        """
        Update self._network_state and invove net_change_callbacks
        @param full_state - A state dict to overwrite the network state
        @param partial_state - A state dict to update the network state
        """
        was_online = self.is_online()
        old_state = self._network_state
        if full_state is not None:
            self._network_state = full_state.copy()
        elif partial_state is not None:
            self._network_state = self._network_state.copy()
            self._network_state.update(partial_state)
        else:
            return
        is_online = self.is_online()
        ip = self.get_ip()
        now = datetime.datetime.utcnow()
        if was_online and not is_online:
            modifiers = [old_state['ip']]
            modifiers += getattr(self, '_ifdn_modifier', [])
            value = int((now - self._last_ifupdn_event).total_seconds())
            self.netlog('IFDN', value=value, modifiers=modifiers)
            self._last_ifupdn_event = now
        elif is_online and not was_online:
            modifiers = [ip]
            modifiers += getattr(self, '_ifup_modifier', [])
            value = int((now - self._last_ifupdn_event).total_seconds())
            self.netlog('IFUP', value=value, modifiers=modifiers)
            self._last_ifupdn_event = now
        elif is_online and ip != old_state.get('ip'):
            self.netlog('IFCHG', modifiers=[ip])
        for callback in self._net_change_callbacks:
            callback(state=self._network_state, old_state=old_state)

    def netlog(self, name, *, modifiers=(), **kwargs):
        """
        Log network statistics, see kaiten.netlog.Logger.event
        """
        try:
            if kaiten.transfer.has_active_transfer():
                modifiers = ['xfer'] + list(modifiers)
            elif self._camera.is_streaming():
                modifiers = ['cam'] + list(modifiers)
            service_hash = self._network_state.get('service_hash')
            if service_hash:
                modifiers = [service_hash] + list(modifiers)
            if self._netlog:
                self._netlog.event(name, modifiers=modifiers, **kwargs)
        except Exception:
            self._log.error("netlog error", exc_info=True)

    def analytics_event(self, action, label=None, value=None):
        """
        DEPRECATED: MIGRATE EVERYTHING TO mixpanel_event
        """
        self._log.error('DEPRECATED CALL TO analytics_event', stack_info=True)

    def mixpanel_event(self, event, client=None, **properties):
        """
        Send an event to MixPanel.

        Only sends the event if we "should" be sending events
        We send events if the user has opted in to analytics,
        but we also send them if the user has not explicitly
        opted out of analytics and has at least one makerbot
        account authorized.

        This also inserts all required global properties.

        @param event: The event name
        @param client: The client directly or indirectly responsible
        @param properties: Event specific properties
        """
        enabled = self._config['kaiten']['analytics_enabled']
        if enabled is False: return
        if enabled is not True:
            # TODO: Maybe we could add a lighter weight check for this?
            if not self.auth_store.get_makerbot_token_map():
                return
        try:
            # Add global properties here
            if client:
                properties.update({
                    'client_type' : client.name,
                    'connection_type' : client.interface         
                })
                username = client.account.username if client.account else None
                if username is not None:
                    properties.update({'username' : username})
            extruder_id = self._machine_manager._pymach.get_tool_id(0)
            if extruder_id != 0:
                tool_stats = self._machine_manager.get_tool_usage_stats()
                properties.update({
                    'extruder_print_time' : tool_stats["extrusion_time_s"],
                    'extruder_id' : tool_stats["serial"],
                    'extruder_version' : mbcoreutils.tool_mappings.Tool.from_id(
                                         extruder_id).pretty_name
                })
            properties.update({
                'printer_serial': kaiten.util.get_serial_dict().get('bot_serial'),
                'source_id': "firmware",
                'printer_id': kaiten.usb.get_serial(),
                'firmware_version' : str(kaiten.util.get_fw_version()),
                'printer_type': self._config['kaiten']['machine_type'],
                'seconds_since_boot' : int(time.time()),
                'first_registered_user': self.auth_store.get_first_registered()
            })

            current_process = self._machine_manager.get_current_process()
            if current_process is not None:
                properties.update({'process_id' : current_process.get_id()})
                properties.update({'process_name' : current_process.get_name()})

            self._mixpanel.event(event, **properties)
        except Exception:
            self._log.error('Error sending analytics', exc_info=True)

    def mixpanel_jsonrpc_callback(self, method, client):
        """
        Generic event that a method has been called.

        We very intentionally do not pass this parameters from the call,
        since knowledge of what to do with parameters should be localized
        to where the method is defined.  Probably we should just
        completely get rid of this generic mechanism.
        """
        event = 'jsonrpc'
        self.mixpanel_event(event, client=client, method=method)

    @kaiten.decorator.jsonrpc
    def network_state(self) -> dict:
        """
        Return the bot's current network state

        @context: Call this all the times

        @return dict: Dictionary containing the network state
        """
        return self._network_state

    def get_machine_info_dict(self):
        """
        This dict is used in handshake() and the udp broadcast.
        This method is invoked in the UDPMulticastServer as a callback.
        """
        machine_dict = {
            "ip": self.get_ip(),
            "port": str(kaiten.constants.tcp_port),
            "ssl_port": str(kaiten.constants.ssl_tcp_port),
            "machine_type": self._config["kaiten"]["machine_type"],
            "machine_name": self._config["kaiten"]["machine_name"],
            "bot_type": self._config["bot_type"],
            "vid": kaiten.usb.get_vid(),
            "pid": kaiten.usb.get_pid(),
            "iserial": kaiten.usb.get_serial(),
            "firmware_version": dict(kaiten.util.get_fw_version()),
            "api_version": kaiten.__version__
        }
        try:
            with open('/var/motor_driver_version', 'r') as f:
                machine_dict['motor_driver_version'] = f.read().strip()
        except Exception as e:
            pass
        return machine_dict

    def _update_udp(self, state=None, old_state=None):
        """
        Make the udp server run only when the internet is up,
        and keep the udp ip address up to date.
        """
        if not self._config["kaiten"]["components"]["udp"]: return
        is_online = self.is_online()
        ip = state.get('ip', None)
        netmask = state.get('netmask', None)
        old_ip = old_state.get('ip', None) if old_state else None
        if is_online and not self.udpserver:
            try:
                self.udpserver = kaiten.udp_server.UDPMulticastServer(
                    ip, netmask, self.get_machine_info_dict)
            except Exception as e:
                self._log.info("%s: Error starting UDP server",
                    self.__class__.__name__, exc_info=True)
            else:
                self._iopoll.register(self.udpserver)
                self._reload_config_callbacks.append(self.udpserver.reload_config)
        elif not is_online and self.udpserver:
            self._iopoll.unregister(self.udpserver)
            self._reload_config_callbacks.remove(self.udpserver.reload_config)
            self.udpserver.stop()
            self.udpserver = None
        elif is_online and ip != old_ip:
            self.udpserver.update_ip(ip, netmask)

    def connect_usb(self, usb_connection):
        self._log.info("Detected a connected usb client")

        # First unregister the USBListener
        self._iopoll.unregister_all(usb_connection)

        # Set up the GeneratorQueue for notifications, etc
        key = get_connection_fileno(usb_connection)
        queue = GeneratorQueue(self._iopoll, usb_connection, False, True)

        # Set up the JsonRPC object
        def send_callback(generator):
            queue.push_back(generator)
        jsonrpc = kaiten.jsonrpc.JsonRpc(
            usb_connection, send_callback, self.mixpanel_jsonrpc_callback, secure=True)
        self._install_jsonrpc(jsonrpc, 2)
        kaiten.jsonrpc.install(jsonrpc, kaiten.usb.UsbMethods(self), 2)

        # Add a wrapper to clean up if the jsonrpc generator terminates
        # TODO: Reinstall a USBListener if this happens
        usb_jsonrpc = self._run_jsonrpc(jsonrpc)
        usb_generator = IOGenerator(usb_jsonrpc, usb_connection, True, False)
        self._clients[key] = kaiten.client.SocketClient(
            key, jsonrpc, usb_generator, queue, 'usb_client', 'usb', 'usb')
        # Set up the USB timeout handler
        timeout_generator = kaiten.usb.USBTimeoutGenerator(
            self, queue, usb_generator)
        def reschedule_callback():
            self.reschedule_contract_generator(timeout_generator)
        queue.set_callback(reschedule_callback)
        self.add_contract_generator(timeout_generator)

        self._iopoll.register(usb_generator)

    def disconnect_usb(self, usb_generator):
        # Cleanup the old connection
        io_error = IOError(19, 'ENODEV', 'Detected USB disconnect')
        self._handle_io_error(usb_generator, io_error)
        usb_generator.connection.close()

        # Install a fresh USBListener to wait for reconnection
        usb_path = kaiten.constants.usb_path
        usb_listener = kaiten.usb.USBListener(self, usb_path)
        self._iopoll.register(usb_listener)

    def jsonrpc_initiate(self, dummy_connection):
        """
        Invokes a JSONRPC method using a dummy connection
        The dummy connection returns a unicode jsonrpc command when read, and writes data to a callback on write
        A JSONRPC object is created, methods are installed into it, and the generator the JsonRPC
        call returns is iterated and as a result the JSONRPC command is executed by kaiten.
        Response data will be written to the dummy_connection write method amd returned to the callback of
        the dummy_connection

        @param dummy_connection: See DummyConnection class in reflector.py
        """
        # iterates the returned JSONRPC generator
        def send_callbacks(generator):
            for dummy in generator:
                 pass

        jsonrpc = kaiten.jsonrpc.JsonRpc(dummy_connection, send_callbacks, 'http_request', 'http')
        self._install_jsonrpc(jsonrpc, 2)
        return jsonrpc

    def jsonrpc_invoke(self, jsonrpc, dummy_connection):
        jsonrpc.feed(dummy_connection.read())

    def send_camera_one_shot_callback(self, callback):
        self._camera.set_one_shot_frame_callback(callback)

   

    def connect_remote(self, remote, initial_priv):
        """
        Opens up a JsonRpc connection to a remote TCP socket.

        @param remote: Address of the remote server in format ip:port
        @param initial_priv: Initial privilege level for this connection

        @return the SocketClient object for this connection.
        """
        # Ideally all objects should be created in factories, preferably ones
        # inherited from Conveyor
        address = kaiten.address.Address.address_factory("tcp:%s" % remote)
        connection = address.connect()
        self._net_socket_manager.register(connection, initial_priv < 2)

        # Create JsonRpc objects and register as an IOGenerator.
        return self._create_client(connection, initial_priv, interface='remote')

    def stop(self, reason=None):
        if reason is None:
            self._log.info("Stopping kaiten server")
        else:
            self._log.info("Stopping kaiten server (%s)", reason)
        self._stop = True
        self._generator_manager.stop()
        self._machine_manager.stop()

    def _do_client_disconnect_hooks(self, client, exc=None, intentional=False):
        for func in self._client_disconnect_hooks:
            func(client, exc, intentional)

    def _cleanup_disconnected_client(self, client, exc=None, intentional=False):
        current_process = self._machine_manager.get_current_process()
        # PrintProcess._transfer_wait is true between the start of the print until
        # the transfer hits the Done section in PrintProcess.transferProgress()
        if None is not current_process:
            if current_process.get_type() == 'printprocess'\
               and current_process._transfer_wait:
                # make sure that it is the client associated with this printprocess
                # and not some other one. the client object stored in current
                # process is actually a jsonprc type client
                if current_process._client == client.jsonrpc:
                    self._log.info("Client dropped while transferring print")
                    current_process.cancel()

    def _log_client_disconnect(self, client, exc=None, intentional=False):
        """
        If this is a network client, log this to the network log.

        Other clients can log straight to the kaiten log.
        """
        jsonrpc = client.jsonrpc
        value = int((datetime.datetime.utcnow()-client.created).total_seconds())
        gen_name = client.get_log_name()
        if client.method == 'network':
            modifiers = []
            if jsonrpc.priv_level > 1: modifiers.append('A')
            if intentional: modifiers.append('I')
            self.netlog('DROP', value=value, modifiers=modifiers, exc=exc)
        elif None is exc:
            self._log.info('Dropping %s (generator complete)', gen_name)
        else:
            self._log_io_drop(gen_name, exc)

    def _log_io_drop(self, name, exc):
        if isinstance(exc, IOError):
            # Sometimes we call this with an IOError that we did not
            # raise.  A full backtrace is too spammy anyway.
            self._log.info('Dropping %s -- %s', name, exc, exc_info=True)
        else:
            self._log.error('Dropping %s -- Unhandled Exception',
                            name, exc_info=True)

    def _handle_io_error(self, io_generator, exc):
        # Raising here will usually crash kaiten, and connections are
        # sometimes in a very odd state here
        try:
            fileno = get_connection_fileno(io_generator.connection)
            self._iopoll.unregister_all(io_generator)
            if fileno in self._clients:
                self._do_client_disconnect_hooks(self._clients[fileno], exc=exc)
                self._clients.pop(fileno)
            elif not getattr(io_generator, 'is_client', False):
                self._log_io_drop('IOGenerator', exc)
        except Exception:
            self._log.error('Unhandleable IO Error', exc_info=True)

    def drop_network_clients(self, username=None, client_to_keep=None):
        """
        Drop all currently connected network clients

        If username is specified, only clients with an account matching
        the given username are dropped.  If client_to_keep is specified,
        it will not be dropped (even if it matches the username).
        """
        count = 0
        for key, client in list(self._clients.items()):
            if client is client_to_keep: continue
            if client.method != 'network': continue
            self._do_client_disconnect_hooks(client, intentional=True)
            self._iopoll.unregister_all(key=key)
            client.jsonrpc.close()
            self._clients.pop(key)
            count += 1
        self._log.info('Dropped %d network clients', count)

    def run(self):
        self._log.info("Running Server")
        code = 0
        try:
            self.play_buzzer("startup")
            self._generator_manager.run()
            self._log.info("Server stopped")
        except InterruptedError as e: # Raised by SIGTERM
            self._log.info("Server interrupted, handled exception")
        except Exception as e:
            self._log.info("Server stopped by unhandled exception", exc_info=True)
            code = -1
            raise
        finally:
            for func in self._cleanup_tasks:
                func()
        return code

    def _register_process_callback(self, process):
        self.add_contract_generator(
            kaiten.machine_manager.ProcessGenerator(self._machine_manager, process),
            process)

    def _install_jsonrpc(self, jsonrpc, priv):
        for installer in self._jsonrpc_installers:
            kaiten.jsonrpc.install(jsonrpc, installer, priv)
        if priv > 1:
            transfer = kaiten.transfer.TransferMethods(self, jsonrpc)
            kaiten.jsonrpc.install(jsonrpc, transfer, 2)
            jsonrpc.store_transfer_object(transfer)

    def _run_jsonrpc(self, jsonrpc):
        """
        This function delegates to the jsonrpc run command.
        """
        key = get_connection_fileno(jsonrpc.connection)
        yield from jsonrpc.run()
        # Cleanup
        if key in self._clients:
            self._do_client_disconnect_hooks(client=self._clients[key])
            self._clients.pop(key)

    def _create_client(self, connection, initial_priv, interface,
                       client_name="tcp", method="network", ssl=False):
        """
        Creates and registers a client object for TCP/Pipe sockets.
        The created client is stored in self._clients and returned

        @param connection: A Socket object
        @param initial_priv: The privilege level this connection starts at
        @param interface: The interface this client is talking through. 'usb',
                          'ethernet', 'wifi', 'tether', or 'pipe'.
        @param client_name: Name for this connection
        @param method: Method of connection for this connection
        @param ssl: Does the connection use an ssl socket?
        """
        key = get_connection_fileno(connection)
        # The GeneratorQueue houses write events, and is used to make sure all
        # write events are executed in a FIFO manner
        queue = GeneratorQueue(self._iopoll, connection, False, True)
        # This callback gets executed when the jsonrpc finishes processing a
        # request and wants to write something back to the socket
        def send_callback(generator):
            queue.push_back(generator)
        # Use initial_priv==3 to identify pipe connections
        secure = ssl or initial_priv == 3
        # make sure we only let tethered clients be tethers
        tethered = interface == 'tether'
        # Set up the JsonRPC object
        jsonrpc = kaiten.jsonrpc.JsonRpc(
            connection, send_callback, self.mixpanel_jsonrpc_callback,
            secure=secure, tether=tethered)
        if ssl:
            jsonrpc.set_ssl(queue)
        self._install_jsonrpc(jsonrpc, initial_priv)
        jsonrpc_generator = self._register_jsonrpc(jsonrpc)
        client = kaiten.client.SocketClient(
            key, jsonrpc, jsonrpc_generator, queue, client_name, method, interface)
        self._clients[key] = client
        return client

    def _register_jsonrpc(self, jsonrpc):
        """
        This function creates an IOGenerator for a JsonRpc object and registers
        the generator with the Server.  Returns the created IOGenerator

        @param jsonrpc: A JsonRpc object
        """
        # Create the IO generator, grabbing the connection off the jsonrpc obj
        generator = IOGenerator(self._run_jsonrpc(jsonrpc), jsonrpc.connection,
            True, False)
        self._iopoll.register(generator)
        return generator

    def _accept_pipe(self, in_listener):
        """
        An infinite generator to accept a socket and set up the various
        IOGenerators and GeneratorQueues.  Unlike _accept_tcp, the
        connection has a privilege level of 3 and we do not limit the
        total number of pipe connections.
        """
        while True:
            self._create_client(in_listener.accept(), 3, interface='pipe',
                                method='pipe')
            yield

    def _accept_tcp(self, in_listener, ssl=False):
        """
        An infinite generator to accept a socket and set up the various
        IOGenerators and GeneratorQueues.  Unlike _accept_pipe, the
        connection starts with a privilege level of 1 and we limit the
        total number of tcp connections.  If ssl is true we wrap the
        connection's socket with ssl.wrap_socket.
        """
        while True:
            connection = in_listener.accept(ssl_wrap=ssl)
            try:
                self._net_socket_manager.register(connection)
            except kaiten.error.TooManySocketsException:
                pass
            else:
                # Grab our network state as our interface
                # should be in ('wifi', 'ethernet', 'offline')
                interface = self._network_state.get('state', '<unknown>')
                if interface != 'wifi' and interface != 'ethernet'\
                   and self.dbus_manager._tethering:
                    # If we're offline, and we're getting a tcp connection, one
                    # would certainly hope it's because we're tethering
                    interface = 'tether'
                self._create_client(connection, 1, interface=interface, ssl=ssl)
            yield

    def state_notification(self):
        self._system_status_generator.state_notification()

    def _notify(self, client, methodname, params, extra = None):
        """
        A wrapper around JsonRpc.notify() that steamrolls most errors

        @param client: The client jsonrpc object to notify
        @param methodname: The name of the method to invoke on the client
        @param params: Params to call the methodname with
        """

        #pass #self._log.debug("Notifying with %s(%r)", methodname, params)
        try:
            yield from client.jsonrpc.notify(methodname, params, extra)
        except ConnectionResetError:
            pass #self._log.debug("Connection reset by peer")
        except kaiten.error.ConnectionWriteException as e:
            pass #self._log.debug("Disconnect at %r", jsonrpc.connection)
        except KeyError as e:
            pass #self._log.debug("Connection %r alread disconnected", jsonrpc.connection)
        except OSError as e:
            if e.errno == errno.EHOSTUNREACH:
                pass #self._log.debug("Connection %r can't reach host", jsonrpc.connection)
            else:
                raise

    def notify_clients(self, methodname, params, limit_id=None):
        """
        Notify all connected clients with sufficient priviledge.  (Skip
        clients which have opted out of notifications).  We very
        specifically do not allow notifications with extra data here.

        We also allow certain classes of notifications to be limited
        to only appear once in a given client's queue.  If a client
        has a pending notification that was passed in with a specific
        limit_id, then a notification passed in with the same limit_id
        will be dropped for that particular client.

        @param methodname: The name of the method to invoke on clients
        @param params: Params to call the methodname with
        @param limit_id: If not None, do not enqueue more than one
            notification with this id to a given client
        """
        for fileno, client in self._clients.items():
            jsonrpc = client.jsonrpc
            if jsonrpc.priv_level < 2:
                # We only want to notify jsonrpc clients with priv >= 2
                continue
            if not client.do_notify:
                continue
            if not client.connected():
                # TODO: can we even get in this state?
                continue
            if limit_id in client.limits:
                continue
            generator = self._notify(client, methodname, params)
            def wrapper(gen, local_client):
                try:
                    yield from gen
                finally:
                    local_client.limits -= set([limit_id])
            if None is not limit_id:
                generator = wrapper(generator, client)
                client.limits |= set([limit_id])
            # Put it in the client queue so it only iterates when there is
            # space in fileno's write buiffer
            client.queue.push_back(generator)

    def _get_client(self, client):
        """
        Verify a client has not disconnected and optionally
        look up client objects by name.

        Currently returns a jsonrpc "client" instead of an actual client
        """
        if not hasattr(client, 'jsonrpc'):
            client_map = {
                "lcd" : self._lcd_client,
                "fcgi" : self._fcgi_client,
            }
            if client_map[client] is not None:
                client = client_map[client]
        if not hasattr(client, 'jsonrpc') or not client.connected():
            raise Exception(getattr(client, 'name', client) + ' not connected')
        return client

    def notify_client(self, client, methodname, params, sent_callback=None, extra=None):
        """
        Notify a specific connected client.  Constructs the notification
        and adds it to the client's send queue.

        @param client: One of "lcd" or "fcgi", or an actual client object
        @param methodname: The name of the method to invoke on the client
        @param params: A dict of params to call the methodname with
        @param sent_callback: Optional "notification sent" callback
        @param extra: Optional extra raw data (see kaiten.jsonrpc.JsonRpc.notify)
        """
        client = self._get_client(client)
        queue = client.queue
        generator = self._notify(client, methodname, params, extra)
        if sent_callback:
            def wrapper():
                try:
                    yield from generator
                finally:
                    sent_callback()
            queue.push_back(wrapper())
        else:
            queue.push_back(generator)

    def make_client_request(self, client, methodname, params, callback, extra=None):
        """
        Make a request on a specific connected client.  Constructs the request
        and adds it to the client's send queue.

        @param client: One of "lcd" or "fcgi", or an actual client object
        @param methodname: The name of the method to invoke on the client
        @param params: A dict of params to call the methodname with
        @param callback: A callback to be invoked when a response is received
        @param extra: Optional extra raw data (see kaiten.jsonrpc.JsonRpc.request)
        """
        client = self._get_client(client)
        request = client.jsonrpc.request(methodname, params, callback, extra)
        client.queue.push_back(request)

    def _init_ssl_context(self):
        class SSLContext(ssl.SSLContext):
            pass  # Fucking __slots__...
        ssl_context = SSLContext(ssl.PROTOCOL_TLSv1)
        pubkeys_path = kaiten.constants.ssl_pub_keys_path
        pubkeys = kaiten.util.read_json_file(pubkeys_path)
        def fix_pubkey(key_dict):
            # The format is a little off of what we want so that
            # printerpanel can use the same pubkeys file
            b64_key = ''.join(key_dict['key'].split('\n')[1:-1])
            # There is also a 24 byte header that we don't use
            return base64.b64decode(b64_key)[24:]
        ssl_context.pinning_dict = {h:fix_pubkey(k) for h,k in pubkeys.items()}
        self._ssl_context = ssl_context

    def check_open_fd_count(self, limit):
        """
        Raise an exception if the current fd count is higher than limit.
        This is a pretty high overhead call, so it should only be attached
        to reasonably infrequent actions.
        """
        # TODO: There might be a better solution using poll() and rlimit
        pid = os.getpid()
        nr_fd = len(os.listdir('/proc/%d/fd' % pid))
        if nr_fd > limit:
            raise Exception('Too many file decriptors')

    def http_request(self, host, path, verb, params=None, https=True, token=None,
                     success_callback=None, error_callback=None,
                     bytes_response=False, file_response=None,
                     encoding=None, read_error_body=False, timeout=60):

        """
        This is the main mechanism for Kaiten to do HTTP or HTTPS requests.
        There are two main paradigms:
        1. Assign a success and/or error callback and dont worry about the
        HTTPIOGenerator object that is returned
        2. Hold on to the HTTPIOGenerator object and yield until complete()
        is true, then call success() and/or response()

        Note that using file_response automatcally implies bytes_response,
        and will prevent the response from being available via self.response
        or the argument passed to success_callback.

        @param host: The host name of the server for the request
        @param path: The path on the server to request
        @param verb: The type of request ('GET', 'POST', etc.)
        @param params: Parameters for GET, otherwise used as the body
        @param https: Whether or not to use https for the request
        @param token: An auth token to include in the request header
        @param success_callback: Invoked with response on success
        @param error_callback: Invoked when the requst fails. Should handle
                               kwargs.
        @param bytes_response: If True, leave the response as raw bytes
        @param file_response: Write the response out to this file object
        @param encoding: Body encoding: 'json', 'url', or None
        @param read_error_body: Whether to read the response body on error
        @param timeout: Timeout in seconds for progress on the request.
                        If we are unable to transfer even a single byte
                        between us and the host for this many seconds,
                        we fail the request.
        """
        params = params or {}
        preexisting_error = None
        ssl_context = None
        try:
            # We currently cap the number of jsonrpc sockets at 128 via another
            # mechanism, so ideally we will only hit this cap when we have too
            # many http related sockets open.
            self.check_open_fd_count(160)

            if https:
                if not self._ssl_context:
                    self._init_ssl_context()
                ssl_context = self._ssl_context
        except Exception as e:
            preexisting_error = e

        httpgen = HTTPIOGenerator(
            self, self._iopoll, host, token, path, verb, params, ssl_context,
            success_callback, error_callback, bytes_response, file_response,
            encoding, read_error_body, timeout, preexisting_error)

        # Don't schedule httpgen until we complete our blocking dns lookup
        def dns_done():
            if False: yield
            # We need to iterate our generator once before registering it
            # because until we start the connection process it is going to
            # poll as disconnected which will make us immediately drop it.
            # This shouldn't ever raise, but if it does ThreadGeneratorQueue
            # should handle it appropriately.
            next(httpgen)
            self._iopoll.register(httpgen)
        def dns_lookup():
            httpgen.do_dns_lookup()
            self._thread_queue.push_back(dns_done())
        if not preexisting_error:
            self._dns_thread.schedule(dns_lookup)
        return httpgen

    def add_contract_generator(self, contract_generator, id=None):
        self._generator_manager.add_contract_generator(contract_generator, id)

    def reschedule_contract_generator(self, id):
        self._generator_manager.reschedule_contract_generator(id)

    def register_status_callback(self, callback):
        self._system_status_generator.register_status_callback(callback)

    @kaiten.decorator.jsonrpc
    def set_staging_urls(self, reflector_url:str=None,
                         thingiverse_url:str=None,
                         drm_print_url:str=None,
                         firmware_update_server_url:str=None,
                         firmware_update_file_url:str=None,
                         firmware_update_list_url:str=None) -> None:
        """
        Points the bot to arbitrary URLs for its web services.
        (e.g. to staging servers)

        @INTERNAL

        @param reflector_url: New URL to point to when talking to the reflector.
                              If None, do not change the setting.

        @param thingiverse_url: New URL to point to when talking to Thingiverse.
                                If None, do not change the setting.

        @param drm_print_url: New URL to point to when doing a DRM print.
                              If None, do not change the setting.
        """
        check = False
        new_config = {'firmware_update': {}}
        if None is not reflector_url:
            new_config['reflector_url'] = reflector_url
        if None is not thingiverse_url:
            new_config['thingiverse_api_url'] = thingiverse_url
        if None is not drm_print_url:
            new_config['drm_server'] = drm_print_url
        if None is not firmware_update_server_url:
            new_config['firmware_update']['firmware_server_url']\
                = firmware_update_server_url
            check = True
        if None is not firmware_update_file_url:
            new_config['firmware_update']['firmware_download_path']\
                = firmware_update_file_url
            check = True
        if None is not firmware_update_list_url:
            new_config['firmware_update']['firmware_versions_list']\
                = firmware_update_list_url
            check = True
        self._log.info("Staging urls: Configuring {0}".format(new_config))
        self.update_home_config(new_config)
        if check:
            self.update_available_firmware(False)

    @kaiten.decorator.jsonrpc
    def reset_staging_urls(self, reset_reflector:bool,
                           reset_thingiverse:bool,
                           reset_drm_print:bool) -> None:
        """
        Resets the machine to default URLs for its web services
        (e.g. from staging servers to production)

        @INTERNAL

        @param reset_reflector: Whether or not to reset reflector url.

        @param reset_thingiverse: Whether or not to reset thingiverse url.

        @param reset_drm_print: Whether or not to reset drm print url.
        """
        new_config = {}
        if reset_reflector:
            new_config['reflector_url'] = kaiten.constants.reflector_url_default
        if reset_thingiverse:
            new_config['thingiverse_api_url']\
                = kaiten.constants.thingiverse_api_url_default
        if reset_drm_print:
            new_config['drm_server'] = kaiten.constants.drm_server_url_default
        self.update_home_config(new_config)

    @kaiten.decorator.jsonrpc
    def get_config(self) -> dict:
        """
        Grabs the bot's entire config and returns it to the invoking client.

        @context: all contexts

        @return dict: config
        """
        return self._config

    def reload_config(self) -> None:
        """
        Reloads the config at run time and execute all callbacks.
        """
        config = kaiten.util.get_config()
        for callback in self._reload_config_callbacks:
            callback(config)

    @kaiten.decorator.jsonrpc
    def set_config(self, config):
        """
        @INTERNAL
        """
        self.update_system_config(config)

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.enforce_types
    def change_machine_name(self, machine_name:str) -> None:
        """
        Changes the machine name to the one passed.

        @param machine_name: New name for the machine
        """
        self.update_home_config({"machine_name": machine_name})
        self.update_discovery()

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.enforce_types
    def set_reflector_enabled(self, enabled:bool):
        """
        Enable/Disable the reflector service

        @param enabled: True to enable, False to disable
        """

        #If the reflector is disabled and we are enabling, enable the component
        #Necessary due to a bug where the reflector component can get stuck
        #disabled for all time (or until a factory reset is done).
        if not self._config["kaiten"]["components"]["reflector"] and enabled:
            self.update_home_config({"components": {"reflector": True}})
            self._setup_reflector()

        self.update_home_config({"reflector_enabled": enabled})

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.enforce_types
    @kaiten.decorator.pass_client
    def set_analytics_enabled(self, enabled:bool, client=None):
        """
        Enable/Disbale bot analytics reporting.

        @param enabled: True to enable, False to disable
        """
        prev_enabled_value = self._config['kaiten']['analytics_enabled']
        if enabled != prev_enabled_value:
            if not enabled:
                self.mixpanel_event("analytics_disabled", client=client)
            self.update_home_config({"analytics_enabled": enabled})
            if enabled:
                self.mixpanel_event("analytics_enabled", client=client)

    @kaiten.decorator.jsonrpc
    def get_cloud_services_info(self) -> dict:
        """
        Returns the current status of cloud services

        The various "enabled" fields of the returned dict indicate whether
        the given service is enabled, with a value of null indicating that
        no one has yet opted in to or out of these services.  The field
        "num_authorized" indicates the total number of authorized accounts
        The field "lcd_username" is present for backward compatibility and
        is always None

        @return: The current status of cloud services
        """
        num_authorized = len(self.auth_store.get_authorized())
        makerbot_tokens = list(self.auth_store.get_makerbot_token_map())

        # If [reflector/analytics] setting is None and we have makerbot tokens in our auth
        # store, then we should show enabled
        reflector_enabled = self._config["kaiten"]["reflector_enabled"]
        if (reflector_enabled is None) and makerbot_tokens:
            reflector_enabled = True

        analytics_enabled = self._config["kaiten"]["analytics_enabled"]
        if (analytics_enabled is None) and makerbot_tokens:
            analytics_enabled = True

        return {
            "reflector_enabled" : reflector_enabled,
            "analytics_enabled" : analytics_enabled,
            "num_authorized": num_authorized,
            "lcd_username" : None,
        }

    @kaiten.decorator.jsonrpc
    def first_contact(self) -> None:
        """
        Will open the user config and mark the first connected flag as True.

        Unlike changing machine name, we DO NOT reload the config.

        We handle this in a strange way because we do not need to reload the
        config after first_connect is called; we just need to change the
        change the server's flag AND the edit the user config.
        """
        self._update_home_config({"has_been_connected_to": True})
        self._has_been_connected_to = True
        # for the honor
        return "jonathan frake's beard"

    def _update_home_config(self, update_dict):
        """
        Update the contents of the kaiten home config on the NAND only
        """
        path = kaiten.constants.kaiten_home_path
        config = kaiten.util.read_json_file(path, ignore_error=True)
        kaiten.util.recursive_update(config, update_dict)
        kaiten.util.write_json_file(path, config)

    def update_home_config(self, update_dict):
        """
        Update and reload the kaiten home config.
        Raise an error if the config cannot be reloaded.
        """
        self._update_home_config(update_dict)
        self.reload_config()

    def _update_machine_config(self, update_dict):
        path = kaiten.constants.config_home_path
        config = kaiten.util.read_json_file(path, ignore_error=True)
        kaiten.util.recursive_update(config, update_dict)
        kaiten.util.write_json_file(path, config)
        
    def update_machine_config(self, update_dict):
        self._update_machine_config(update_dict)
        self.reload_config()

    def update_system_config(self, update_dict):
        new_kaiten_settings = update_dict.pop('kaiten', None)
        self._update_machine_config(update_dict)
        if new_kaiten_settings:
            self._update_home_config(new_kaiten_settings)
        self.reload_config()

    @kaiten.decorator.lower_priv
    @kaiten.decorator.jsonrpc
    def handshake(self, username:str=None, host_version:str=None) -> dict:
        """
        Called by a client to determine if the machine can be communicated with.

        @context: Use this during initial communication with a bot

        @param username: DEPRECATED - required but unused by pre 1.0.0 apis
        @param host_version: DEPRECATED - required but unused by pre 1.0.0 apis
        @return dict: A bunch of information about the machine
        """
        return self.get_machine_info_dict()

    @kaiten.decorator.lower_priv
    @kaiten.decorator.jsonrpc
    def ping(self) -> bool:
        """
        Function that returns True to the client, for pinging the bot.

        This funciton is used by the host service to make sure this machine
        is still up and running.  Its called every couple seconds.

        @context: Used periodically by clients, too see if the connection is
                  still alive

        @return DEPRECATED returns True when called, clients should accept True or null
        """
        return True

    def authenticate(self, client):
        """
        Authenticates a client to priv_level 2

        This unconditionally authenticates the provided client, and
        should not be confused with kaiten.auth.AuthStore.authenticate,
        which is the jsonrpc exposed method which only authenticates a
        client if it has been authorized.
        """
        jsonrpc = client.jsonrpc
        if jsonrpc.priv_level >= 2: return
        self._install_jsonrpc(jsonrpc, 2)
        self._net_socket_manager.persist(jsonrpc.connection)
        # Adding a persistent buffer improves file transfer speeds for
        # network connections, but it also consumes memory for each
        # connection.  So we do not allocate it until we authenticate
        jsonrpc.connection.set_buffer(65535) # TCP window size?
        self._log.info("Accepted authentication for connection")

    @kaiten.decorator.jsonrpc
    def sync_account_to_bot(self):
        """ DEPRECATED - Does nothing """
        pass

    @kaiten.decorator.jsonrpc
    def desync_account(self):
        """ DEPRECATED - Does nothing """
        pass

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.pass_client
    def expire_thingiverse_credentials(self, client=None) -> None:
        """
        Remove the account from the current client
        """
        client.account = None

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.pass_client
    @kaiten.decorator.require_secure
    def set_thingiverse_credentials(self, thingiverse_username:str,
                                    thingiverse_token:str, client=None) -> None:
        """
        Sets thingiverse information for the current client.

        DEPRECATED: Use add_makerbot_account + reauthorize instead

        @context: Not available over unsecure channels.

        @param thingiverse_username: Valid thingiverse username to be set
        @param thingiverse_token: Token to be associated with the given client
        """
        self.auth_store.add_makerbot_account(
            thingiverse_username, thingiverse_token)
        self.auth_store.set_account(client, thingiverse_username)

    @kaiten.decorator.raise_priv
    @kaiten.decorator.pass_client
    @kaiten.decorator.jsonrpc
    def register_lcd(self, client) -> bool:
        """
        Register the LCD client with kaiten.

        @context: Should only be called by PrinterPanel

        @return bool: True if successful
        """
        self._lcd_client = client
        client.name = 'printerpanel'
        client.method = 'printerpanel'

        # If we are running a process when the UI client connects,
        # the UI client doesn't notice.  We should probably fix the
        # UI client, but this is way easier.
        self.notify_client(
            client, 'state_notification',
            {'info': self._system_status_generator.get_info_dict()})

        return True

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.pass_client
    def register_client_name(self, name:str, client=None):
        """
        Register a name with the given client.

        @param name: Name to identify the client      
        """
        client.name = name

    @kaiten.decorator.raise_priv
    @kaiten.decorator.pass_client
    @kaiten.decorator.jsonrpc
    def register_fcgi(self, client):
        """
        Registers the FCGI as a client.

        Also make sure that _update_udp is called if/when the network is up

        @context: Should only be called by the FCGI server
        """
        client.do_notify = False
        self._fcgi_client = client
        client.name = 'fcgi'
        self._log.info('Registered fcgi')
        if not self._fcgi_up:
            self._fcgi_up = True
            self.add_net_change_callback(self._update_udp)
            if self.is_online():
                self._update_udp(state=self._network_state)

    @kaiten.decorator.jsonrpc
    def get_system_information(self) -> dict:
        """
        Returns the full system info dict.

        @context: all contexts

        @return dict: System information dictionary
        """
        return self._system_status_generator.get_info_dict()

    @kaiten.decorator.jsonrpc
    def capture_image(self, output_file:str) -> None:
        """
        Captures an image to output_file.

        Will block all notifications and responses to the client until the
        file has been saved.

        @context: This should be used when the bot is idle or extremely sparingly
                  during a process. Excessive use of this may cause the client
                  to be disconnected.

        @param output_file: Path to put the outputted jpeg
        """
        # We currently do not have any explicit resource management code here.
        # Because this is iterated by the clients own send queue, the risk of
        # a client spamming this is mitigated by the contract manager itself.
        yield from self._camera.save_jpeg(output_file)
        return True

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.pass_client
    def request_camera_frame(self, client=None) -> bool:
        """
        Request that a single frame be sent to the client.

        Via the "camera_frame" raw mode notification.

        @context: all contexts

        @return bool: Returns True if a notification will actully be sent.
        """
        return self._camera.request_frame(client)

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.pass_client
    def request_camera_stream(self, client=None):
        """
        Subscribe a client to a stream of camera_frame raw mode notifications.

        @context: all contexts
        """
        self._camera.request_stream(client)

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.jsonrpc_immediate
    @kaiten.decorator.pass_client
    def end_camera_stream(self, client=None):
        """
        Unsubscribe a client to a stream of camera_frame raw mode notifications.

        @context: Only makes sense to call if the client was subscribed to the
                  camera stream
        """
        self._camera.end_stream(client)

    @kaiten.decorator.raise_priv
    @kaiten.decorator.jsonrpc
    def set_namuga_resolution(self, width:int, height:int):
        """
        Changes the camera resolution for the namuga camera

        This has no effect on the older style of cameras, other than closing
        and reopening the camera device.  The new settings will not persist
        after reboot -- this is currently just intended for testing out
        various settings via the onboard repl.  If we want to actually expose
        this as an option to clients it would have to be reimplemented in a
        less hacky fashion.

        @INTERNAL

        @param width: The width of camera frames in pixels
        @param height: The height of camera frames in pixels
        """
        settings = kaiten.bwcamera.SUPPORTED_CAMERAS['NMG HD WebCam']
        settings.width = width
        settings.height = height
        self._camera.close()

    @kaiten.decorator.raise_priv
    @kaiten.decorator.jsonrpc
    def transfer_progress(self, local_path:str, progress:int, done:bool=False):
        """
        Updates the transfer progress of a file transfer.

        If any processes have registered an interest in file transfers to a
        given local_path, notify then via the callbacks they have provided.

        @context: Called during a file transfer

        @param local_path: Path to the file we want progress for
        @param progress: int representing the progress of the transfer
        @param done: If True the transfer is done
        """
        local_path = os.path.normpath(local_path)
        if local_path in self._transfer_callbacks:
            for callback in self._transfer_callbacks[local_path][:]:
                try:
                    callback(progress, done)
                except Exception:
                    self._log.error('Evicting transfer callback', exc_info=True)
                    self._transfer_callbacks[local_path].remove(callback)
            if done:
                del self._transfer_callbacks[local_path]

    def add_transfer_callback(self, local_path, callback):
        """
        callback will be invoked on every progress update for local_path,
        with arguments of progress (an int between 0 and 100) and done (bool)
        until it is invoked with done=True, or until the callback raises
        an error, or until clear_transfer_callbacks(file_path) is invoked.

        Be careful with the callbacks you install since if none of the above
        three conditions is ever encountered, the callback will never ever go
        away.  At some point this needs to be rethought.
        """
        local_path = os.path.normpath(local_path)
        if local_path not in self._transfer_callbacks:
            self._transfer_callbacks[local_path] = []
        self._transfer_callbacks[local_path].append(callback)

    def clear_transfer_callbacks(self, local_path):
        """
        Remove all callbacks that are waiting for information from
        local_path.
        """
        if local_path in self._transfer_callbacks:
            del self._transfer_callbacks[local_path]

    @kaiten.decorator.jsonrpc
    def birdwing_list(self, path:str) -> list:
        """
        Recursively list all the files in a directory.

        This is needed for listing over USB

        @context: Used mostly for file browsing over USB

        @param path: Path we want to return the files under
        @return list: List of files under the give path
        """
        # chroot the path to home_dir
        path = kaiten.util.chroot_path(kaiten.constants.home_dir, path)
        results = []
        for root, subdirs, files in os.walk(path):
            for filename in files:
                filepath = os.path.join(root, filename)
                results.append(
                    os.path.relpath(filepath, kaiten.constants.home_dir))
        return results

    def play_buzzer(self, song:str):
        """ Play the specified song if the user has sound enabled. """
        if self._config['kaiten']['sound']:
            kaiten.util.play_buzzer(song)

    @kaiten.decorator.jsonrpc
    def update_available_firmware(self, only_notify:bool=True):
        """
        Check for new firmware.

        If only_notify is true we just resend the notification for
        the result from the last check.  This defaults to true to
        support clients that were written to call this method
        automatically on connection, but for forward compatibility
        this should be explicitly passed.

        @param only_notify: Whether to check or just resend results
        """
        self.firmware_updates.check(only_notify)

    @kaiten.decorator.jsonrpc
    @kaiten.decorator.raise_priv
    def restart_ui(self):
        """
        Restart the UI client

        Does not wait until the client has started up again.  Intended to
        be invoked directly by the UI client.
        """
        cmd = ['/etc/init.d/S07BotUI', 'restart']
        # We probably aren't going to finish this generator because the
        # client connection that is iterating this is generally the ui
        # client that we are restarting.
        yield from kaiten.util.subprocess_call(cmd)

class HTTPIOGenerator(IOGenerator):

    def __init__(self, server, iopoll_generator, host, token,
                 path, verb, params, ssl_context,
                 success_callback=None,
                 error_callback=None,
                 bytes_response=False,
                 file_response=None,
                 encoding=None,
                 read_error_body=False,
                 timeout=None,
                 preexisting_error=None):
        """
        iopoll_generator is the IOPollGenerator object this generator
        is registered with, and is used here to switch from a write
        generator to a read generator.  For a description of all other
        parameters, see Server.http_request.

        The preexisting_error can be used to generate a request object
        that will generally look like a normal request but will just
        fail after a short delay.
        """

        self._log = kaiten.log.getlogger(self)
        self._server = server

        if ssl_context:
            self._conn = kaiten.http.client.HTTPSConnection(host, context=ssl_context)
        else:
            self._conn = kaiten.http.client.HTTPConnection(host)
        if not preexisting_error:
            self._sock = self._conn.create_socket()

        self._host = host
        self._success_callback = success_callback
        self._error_callback = error_callback
        self._bytes_response = bytes_response
        self._file_response = file_response
        self._encoding = encoding
        self._read_error_body = read_error_body

        self._iopoll = iopoll_generator

        self._complete = False
        self._success = False
        self._response = ""
        self._response_code = None
        self._http_response = None
        self._error_body = None

        params = params or {}
        if preexisting_error:
            self._defer_error(preexisting_error)
        else:
            generator = self._create_generator(token, path, verb, params)
            if timeout:
                generator = self._timeout_wrapper(generator, timeout)
            super().__init__(generator, self._sock, False, True)

    def complete(self):
        return self._complete

    def success(self):
        return self._success

    def response(self):
        return self._response

    def progress(self):
        """
        Returns download progress (0-100)
        """
        if self._http_response:
            return 100 * self._http_response.download_progress()
        else:
            return 0

    def _error(self, error, *args, **kwargs):
        self._complete = True
        self._conn.close()
        self._log.error(error, *args, **kwargs)
        if self._error_callback:
            kwargs = {'response_code': self._response_code}
            if self._read_error_body:
                kwargs['error_body'] = self._error_body
            self._error_callback(**kwargs)

    def _set_read(self):
        """ Update eventmask for reading and not writing """
        self._is_write = False
        self._is_read = True
        self._iopoll.update_eventmask(self)

    def _set_write(self):
        """ Update eventmask for reading and not writing """
        self._is_read = False
        self._is_write = True
        self._iopoll.update_eventmask(self)

    def do_dns_lookup(self):
        """
        This must be invoked BEFORE we start iterating.  It also can
        block indefinitely, so it must be invoked from a seperate thread.
        """
        self._conn.do_dns_lookup()

    def expected_run_time(self):
        # this generator routinely blocks for over one second
        return datetime.timedelta(seconds=2)

    def _create_generator(self, token, path, verb, params=None, debug=False):
        if debug:
            self._conn.set_debuglevel(1)
        try:
            for event in self._conn.connect():
                if event == 'want_read':
                    self._set_read()
                elif event == 'want_write':
                    self._set_write()
                yield
        except Exception as e:
            self._error("HTTP error connecting", exc_info=True)
            return
        if self._is_read:
            self._set_write()
        headers = {}
        if token:
            headers["Authorization: Bearer"] = token
        # if we are not doing a GET (e.g. POST, PATCH) then use params
        # as the body with the specified encoding
        params = params or {}
        if verb != "GET":
            if self._encoding == 'json':
                headers["Content-Type"] = "application/json"
                body = json.dumps(params)
            elif self._encoding == 'url':
                headers["Content-Type"] = "application/x-www-form-urlencoded"
                if isinstance(params, dict):
                    body = urllib.parse.urlencode(params)
                else:
                    # Assume the caller has handled encoding
                    body = params
            else:
                headers["Content-Type"] = "application/octet-stream"
                body = params
        else:
            if params:
                path += '?' + urllib.parse.urlencode(params)
            body = None
        try:
            yield from self._conn.request(verb, path,
                body=body, headers=headers)
        except Exception as e:
            self._error("HTTP  request error", exc_info=True)
            return

        self._set_read()

        try:
            self._http_response = yield from self._conn.getresponse()
        except Exception as e:
            self._error("Error getting HTTP response", exc_info=True)
            return
        # check for a valid code
        self._response_code = self._http_response.status
        if self._response_code not in [kaiten.http.client.OK,
                                       kaiten.http.client.CREATED,
                                       kaiten.http.client.ACCEPTED]:
            if self._read_error_body:
                try:
                    self._error_body = yield from self._http_response.read()
                except Exception:
                    # Don't call error here since we call it below
                    self._log.error("Could not read error body", exc_info=True)
            self._error("HTTP error response: %d, Request: %s %s %s, X-Error: %s",
                self._response_code, self._host, verb, path,
                self._http_response.getheader("X-Error", "None"))
            return
        if self._bytes_response:
            self._http_response.decode = False
        try:
            if self._file_response:
                yield from self._http_response.pipe(self._file_response)
            else:
                self._response = yield from self._http_response.read()
        except Exception as e:
            self._error("Error reading HTTP response", exc_info=True)
            return
        self._complete = True
        self._success = True

        # fire success callback
        if self._success_callback:
            self._success_callback(self._response)

    def _timeout_wrapper(self, generator, timeout):
        # This itself is a generator that yields from generator, but
        # first it sets up a timeout contract, and every time generator
        # yields it defers the contract.
        done = False
        @kaiten.decorator.contractify(interval=timeout)
        def trigger_timeout(self):
            if False: yield
            if not done:
                self._error("HTTP request timed out")
                generator.close()
        timeout_contract = trigger_timeout(self)
        def defer_timeout():
            self._server.reschedule_contract_generator(timeout_contract)

        # It turns out that correctly implementing yield from is hard
        # This is based on PEP 380, but simplified knowing that generator
        # is already a generator, and that we never call throw, only next
        # send and close.
        try:
            _y = next(generator)
        except StopIteration:
            pass
        else:
            while True:
                defer_timeout()
                try:
                    _s = yield _y
                except GeneratorExit as _e:
                    generator.close()
                    raise _e
                else:
                    try:
                        if _s is None:
                            _y = next(generator)
                        else:
                            _y = generator.send(_s)
                    except StopIteration:
                        done = True
                        break

    @kaiten.decorator.contractify(interval=1)
    def _defer_error(self, error):
        yield
        self._error(error)


class GeneratorQueue(IOGenerator):
    """
    A queue of generators.  Generators can be appended to this queue to ensure
    they are iterated in a FIFO order.  All notifications are added to this queue
    instead of directly onto the GeneratorManager.  If the queue grows too big
    we close down the entire connection.

    A callback can also be installed which is called on every iteration and also
    when the queue transitions from empty to nonempty.
    This should be used with great discretion.
    """
    def __init__(self, iopoll, connection, reader, writer):
        self._queue = []
        self._iopoll = iopoll
        super(GeneratorQueue, self).__init__(self, connection, reader, writer)
        self._log = kaiten.log.getlogger(self)
        self._callback = None
        self._overflow = False

    def set_callback(self, callback):
        self._callback = callback

    def push_back(self, generator):
        """
        Appends a generator to the back of the queue.  We only want to register
        ourselves ONCE, so we only register if the len of the queue is 1 (i.e.
        at the beginning)
        """
        if self._overflow: return
        self._queue.append(generator)
        # This is weird, see function docstring for explanation
        if len(self._queue) == 1:
            if self._callback: self._callback()
            self._iopoll.register(self)
        elif len(self._queue) > kaiten.constants.client_packet_limit:
            self._log.error("Dropping client due to send queue overflow")
            self._overflow = True
            self.connection.close()
            self._queue = []

    def _pop_front(self):
        """
        Pops a generator off the queue.  Only unregister ourself when the length
        of the queue is 0, since that means we're totally done
        """
        self._queue = self._queue[1:]
        # This is weird, see function docstring for explanation
        if len(self._queue) is 0:
            self._iopoll.unregister(self)

    def __len__(self):
        return len(self._queue)

    def __next__(self):
        """
        Calls next on the first generator.  If there are no generators to
        iterate, we simply pass.  If we exhaust a generator, we pop it off.
        """
        if self._overflow:
            raise IOError(errno.EOVERFLOW)
        if self._callback: self._callback()
        try:
            next(self._queue[0])
        except IndexError as e:
            pass
        except StopIteration as e:
            self._pop_front()
        except IOError as e:
            # If we have a ConnectionWriteExcepion, our pipe is broken and
            # we should unregister ourselves
            self._log.info("IOError during iteration: %s", str(e), exc_info=True)
            # This will unregister this generator with everything
            raise e
        except TypeError as e:
            self._log.info("Generator {} isn't an iterator"
                           .format(self._queue[0].__name__))
            self._pop_front()
        except Exception as e:
            self._log.info("Generic Error iterating generator", exc_info=True)
            self._pop_front()

class ThreadGeneratorQueue(IOGenerator):
    """
    A generator which allows a slave thread to queue events for the
    IOPollGenerator to execute.  Once this is installed, any generator
    passed to push_back() will be executed by IOPollGenerator. push_back()
    must only be called by slave threads since there is a nonzero chance
    that it will block.
    """
    def __init__(self):
        self._log = kaiten.log.getlogger(self)

        # Internally we use a pipe to make sure we block when no generators
        # are pending.  We write one character to the pipe for each added
        # generator and read one character for each exhausted generator
        self._pipe = None
        self._pipe = os.pipe()

        # All thread safety is provided by using an actual queue
        self._queue = Queue.Queue()
        self._head = None

        # Mildly hacky stuff in lieu of calling IOGenerator.__init__
        self.connection = self
        self._is_read = True
        self._is_write = False

    def __del__(self):
        self.connection = None
        if self._pipe:
            os.close(self._pipe[0])
            os.close(self._pipe[1])
            self._pipe = None

    def fileno(self):
        return self._pipe[0]

    def push_back(self, generator):
        self._queue.put(generator)
        os.write(self._pipe[1], b'x')

    def __next__(self):
        if not self._head:
            try:
                self._head = self._queue.get_nowait()
            except Exception:
                self._log.info('%s false trigger', self.__class__.__name__, exc_info=True)

        try:
            next(self._head)
        except StopIteration:
            pass
        except Exception:
            self._log.info('Generator exception', exc_info=True)
        else:
            return

        # Done with the generator, so evict it and remove a token from the pipe
        self._head = None
        os.read(self._pipe[0], 1)

class ExtruderStatusGenerator(object):
    """
    System status has its own generator, since it needs to have a separate
    contract than machine manager.  Otherwise, we would constantly be trying
    so send system notifications, which clogs up the various IO queues and
    blocks kaiten from writing out to sockets on time.

    Even though System and state notifications appear similar, we are not going
    to colllapse them into one notification.  If we did, the constant subtle
    system changes (i.e. temp, etc) would cause too many IO events and bog down
    the machine.
    """

    def __init__(self, server):
        self._server = server
        self._get_status_callbacks = []

    def register_status_callback(self, callback):
        self._get_status_callbacks.append(callback)

    def contract_duration(self):
        return datetime.timedelta(0, 1.5)

    def expected_run_time(self):
        return datetime.timedelta(seconds=kaiten.constants.normal_generator_time)

    def get_info_dict(self):
        status_dict = {}
        for callback in self._get_status_callbacks:
            status_dict.update(callback())
        status_dict.update({"api_version": kaiten.__version__})
        return status_dict

    def state_notification(self):
        """
        Force a state_notification to be sent out.
        They reset the system notification contract.
        """
        machine_status = self.get_info_dict()
        method = "state_notification"
        params = {"info": machine_status}
        self._server.notify_clients(method, params)
        # We need to deepcopy, otherwise we'll be stuck with a reference to
        # what we want to compare to
        # reset contract for system_notifications
        self._server.reschedule_contract_generator(self)

    def __next__(self):
        machine_status = self.get_info_dict()
        method = "system_notification"
        params = {"info": machine_status}
        self._server.notify_clients(method, params, limit_id=self)

class DataRateLimiter(object):
    def __init__(self, rate, smoothing_delay=0.7):
        """
        @param rate: Maximum data rate in bytes per second
        @param smoothing_delay: See comments, unit is seconds
        """
        self._rate = rate
        self._smoothing_delay = datetime.timedelta(seconds=smoothing_delay)
        self._log = kaiten.log.getlogger(self)
        self._next_send = datetime.datetime.utcnow()

    def try_data(self, byte_count):
        """
        Determine whether the given number of bytes can be sent without
        exceeding our data rate.  Assumes that the data will actually be
        sent if True is returned.
        """
        now = datetime.datetime.utcnow()
        if now < self._next_send:
            return False
        time_step = datetime.timedelta(seconds=(byte_count/self._rate))
        # Just using self._next_send + time_step would be the most accurate
        # limiting formula, but it would allow a client to build up an
        # arbitrarily large data balance when they were using less than the
        # limit.  Using now + time_step prevents this, but using only this
        # is inaccurate.  The following formula should cap the data balance
        # that can be build up to rate*smoothing_delay, and should remain
        # accurate as long as smoothing_delay exceeds the rate at which
        # try_send is invoked.
        self._next_send = max([
            self._next_send + time_step,
            now + time_step - self._smoothing_delay,
        ])
        return True

class DNSThread(threading.Thread):
    """
    Just a seperate thread for executing functions that take no arguments.
    Currently only used for DNS, hence the name.
    """
    _SENTINEL = object()

    def __init__(self):
        self._log = kaiten.log.getlogger(self)
        self._queue = Queue.Queue()
        super().__init__()

    def schedule(self, func):
        self._queue.put(func)

    def stop(self):
        self._queue.put(DNSThread._SENTINEL)

    def run(self):
        while True:
            func = self._queue.get()
            if func is DNSThread._SENTINEL:
                break
            try:
                func()
            except Exception:
                self._log.error('Exception in DNSThread', exc_info=True)

